import requests
import re
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from telegram import InlineKeyboardButton, InlineKeyboardMarkup, Update, ReplyKeyboardMarkup
from telegram.ext import Application, CommandHandler, MessageHandler, filters, CallbackQueryHandler, ContextTypes, ConversationHandler

# ---------- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ----------
TOKEN = "Ø¶Ø¹_ØªÙˆÙƒÙ†_Ø§Ù„Ø¨ÙˆØª_Ù‡Ù†Ø§"
BASE_URL = "https://weciema.video"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0 Safari/537.36",
    "Accept-Language": "ar,en-US;q=0.9,en;q=0.8"
}

# Ù…Ø±Ø§Ø­Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
SEARCH, RESULTS = range(2)


async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    keyboard = [["ğŸ” Ø¨Ø­Ø«"]]
    reply_markup = ReplyKeyboardMarkup(keyboard, resize_keyboard=True)
    await update.message.reply_text("ğŸ‘‹ Ù…Ø±Ø­Ø¨Ù‹Ø§! Ø§Ø¶ØºØ· Ø²Ø± ğŸ” Ø«Ù… Ø§ÙƒØªØ¨ Ø§Ø³Ù… Ø§Ù„ÙÙŠÙ„Ù… Ø£Ùˆ Ø§Ù„Ù…Ø³Ù„Ø³Ù„.", reply_markup=reply_markup)
    return SEARCH


def _safe_get(url, headers=HEADERS, use_cloudscraper=False, timeout=12):
    """
    Ù…Ø­Ø§ÙˆÙ„Ø© Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø© Ø£ÙˆÙ„Ø§Ù‹ Ø¨Ù€ requests Ù…Ø¹ Ø±Ø¤ÙˆØ³ Ø§Ù„Ù…ØªØµÙØ­ØŒ
    ÙˆØ¥Ù† Ù„Ù… ØªÙ†Ø¬Ø­ (Ù…Ø«Ù„ Ø­Ù…Ø§ÙŠØ© Cloudflare)ØŒ ÙŠÙ…ÙƒÙ† ØªÙØ¹ÙŠÙ„ cloudscraper Ø¹Ù† Ø·Ø±ÙŠÙ‚ use_cloudscraper=True.
    """
    try:
        if use_cloudscraper:
            import cloudscraper
            sc = cloudscraper.create_scraper()
            r = sc.get(url, headers=headers, timeout=timeout)
        else:
            r = requests.get(url, headers=headers, timeout=timeout)
        return r
    except Exception as e:
        # Ø§Ø±Ø¬Ø¹ None Ø¹Ù†Ø¯ Ø§Ù„ÙØ´Ù„
        return None


def extract_iframe_from_html(html, base=BASE_URL):
    """Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‚ÙŠÙ…Ø© iframe Ø£Ùˆ data-src Ø£Ùˆ src Ø¶Ù…Ù† Ø³ÙƒØ±Ø¨Øª"""
    soup = BeautifulSoup(html, "html.parser")
    # Ù…Ø¨Ø§Ø´Ø±Ø© iframe
    iframe = soup.find("iframe")
    if iframe and iframe.get("src"):
        return urljoin(base, iframe["src"])

    # Ø¨Ø­Ø« Ø¨Ø§Ù„Ù€ regex Ø¯Ø§Ø®Ù„ Ø§Ù„Ù€ HTML
    m = re.search(r'iframe[^>]+src=["\']([^"\']+)["\']', html)
    if m:
        return urljoin(base, m.group(1))

    m2 = re.search(r'data-src=["\']([^"\']+)["\']', html)
    if m2:
        return urljoin(base, m2.group(1))

    # Ø¨Ø¹Ø¶ Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø© ØªØ¶Ø¹ embed Ø¯Ø§Ø®Ù„ Ø³ÙƒØ±Ø¨Øª
    m3 = re.search(r'src:\s*["\']([^"\']+)["\']', html)
    if m3:
        return urljoin(base, m3.group(1))

    return None


async def search_request(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("ğŸ” Ø£Ø±Ø³Ù„ Ø§Ø³Ù… Ø§Ù„ÙÙŠÙ„Ù… Ø£Ùˆ Ø§Ù„Ù…Ø³Ù„Ø³Ù„ Ù„Ù„Ø¨Ø­Ø«:")
    return RESULTS


async def search_results(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.message.text.strip()
    await update.message.reply_text(f"ğŸ” Ø£Ø¨Ø­Ø« Ø¹Ù†: {query} ...")

    # Ø£ÙˆÙ„ Ù…Ø­Ø§ÙˆÙ„Ø©: Ø§Ø³ØªØ®Ø¯Ø§Ù… ØµÙØ­Ø© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ©
    search_url = f"{BASE_URL}/?s={query.replace(' ', '+')}"
    r = _safe_get(search_url)
    # Ø¥Ø°Ø§ Ø§Ù„ØµÙØ­Ø© ÙØ§Ø¶ÙŠØ© Ø£Ùˆ Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹ØŒ Ù†Ø¬Ø±Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… cloudscraper
    if not r or r.status_code != 200 or len(r.text) < 800:
        r = _safe_get(search_url, use_cloudscraper=True)

    if not r or r.status_code != 200:
        # fallback: Ø­Ø§ÙˆÙ„ Ø§Ù„Ø¨Ø­Ø« Ø¯Ø§Ø®Ù„ ØµÙØ­Ø§Øª Ø±Ø¦ÙŠØ³ÙŠØ©/ÙØ¦Ø§Øª (home, series, movies)
        candidates = []
        scan_pages = [f"{BASE_URL}/home/", f"{BASE_URL}/series/", f"{BASE_URL}/movies/"]
        for p in scan_pages:
            rp = _safe_get(p)
            if not rp or rp.status_code != 200:
                rp = _safe_get(p, use_cloudscraper=True)
            if not rp:
                continue
            soup_p = BeautifulSoup(rp.text, "html.parser")
            for a in soup_p.find_all("a", href=True):
                txt = a.get_text(strip=True)
                if txt and query.lower() in txt.lower():
                    href = urljoin(BASE_URL, a['href'])
                    candidates.append((txt, href))
        if not candidates:
            await update.message.reply_text("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬. Ø¬Ø±Ù‘Ø¨ ÙƒÙ„Ù…Ø© Ù…Ø®ØªÙ„ÙØ© Ø£Ùˆ ØªØ£ÙƒØ¯ Ù…Ù† Ø§ØªØµØ§Ù„ Ø§Ù„Ù€ VPS/Replit Ø¨Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª.")
            return SEARCH

        # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ù† Ø§Ù„Ù€ fallback
        keyboard = []
        seen = set()
        for title, href in candidates[:40]:
            if href in seen: continue
            seen.add(href)
            keyboard.append([InlineKeyboardButton(title, callback_data=f"item|{href}")])
        await update.message.reply_text("ğŸ“Œ Ø§Ø®ØªØ± Ø§Ù„Ø¹Ù…Ù„:", reply_markup=InlineKeyboardMarkup(keyboard))
        return ConversationHandler.END

    # Ø§Ù„Ø¢Ù† parsing ØµÙØ­Ø© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„ÙØ¹Ù„ÙŠØ©
    soup = BeautifulSoup(r.text, "html.parser")

    # Ø­Ø§ÙˆÙ„ selectors Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ø£ÙˆÙ„Ø§Ù‹
    results = soup.select(".ml-item a") or soup.select(".list-film a") or soup.select("article a")
    # Ù„Ùˆ Ù„Ù… Ù†Ø¬Ø¯ØŒ Ø§Ø¨Ø­Ø« Ø¹Ù† Ø£ÙŠ <a> ÙŠØ­ØªÙˆÙŠ Ù†Øµ Ø§Ù„Ø¨Ø­Ø«
    if not results:
        results = []
        for a in soup.find_all("a", href=True):
            txt = a.get_text(strip=True)
            if txt and query.lower() in txt.lower():
                results.append(a)

    if not results:
        await update.message.reply_text("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ (Ù„Ù… Ø£Ø¬Ø¯ Ø¹Ù†Ø§ØµØ± Ù…Ø·Ø§Ø¨Ù‚Ø©).")
        return SEARCH

    keyboard = []
    seen = set()
    for item in results:
        title = item.get("oldtitle") or item.get_text(strip=True) or "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"
        url = urljoin(BASE_URL, item["href"])
        if url in seen: continue
        seen.add(url)
        keyboard.append([InlineKeyboardButton(title, callback_data=f"item|{url}")])

    if not keyboard:
        await update.message.reply_text("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø¹Ø±Ø¶.")
        return SEARCH

    await update.message.reply_text("ğŸ“Œ Ø§Ø®ØªØ± Ø§Ù„Ø¹Ù…Ù„:", reply_markup=InlineKeyboardMarkup(keyboard))
    return ConversationHandler.END


async def button_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    data = query.data.split("|", 1)
    typ = data[0]
    url = data[1]

    # Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø© (Ù…Ø¹ fallback Ù„Ù„Ù€ cloudscraper)
    r = _safe_get(url)
    if not r or r.status_code != 200 or len(r.text) < 400:
        r = _safe_get(url, use_cloudscraper=True)
    if not r:
        await query.message.reply_text("Ø®Ø·Ø£ Ø¹Ù†Ø¯ Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø©. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ù„Ø§Ø­Ù‚Ù‹Ø§.")
        return

    soup = BeautifulSoup(r.text, "html.parser")

    # Ø¥Ø°Ø§ Ø§Ù„ØµÙØ­Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø­Ù„Ù‚Ø§Øª (episodes) -> Ø§Ø¬Ù„Ø¨ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªÙŠ ØªØ­ÙˆÙŠ ÙƒÙ„Ù…Ø© 'Ø­Ù„Ù‚Ø©' Ø£Ùˆ Ø±ÙˆØ§Ø¨Ø· Ø¯Ø§Ø®Ù„ Ù‚Ø³Ù… Ø§Ù„Ø­Ù„Ù‚Ø§Øª
    ep_links = []
    # Ø§Ø¨Ø­Ø« Ø£ÙˆÙ„Ø§ Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· ØªØ­ØªÙˆÙŠ ÙƒÙ„Ù…Ø© Ø­Ù„Ù‚Ø© Ø¨Ø§Ù„Ù€ text
    for a in soup.find_all("a", href=True):
        txt = a.get_text(strip=True)
        if txt and ("Ø­Ù„Ù‚Ø©" in txt or "episode" in txt.lower() or "ep" in txt.lower()):
            href = urljoin(BASE_URL, a['href'])
            ep_links.append((txt, href))

    # Ø¥Ù† ÙˆÙØ¬Ø¯Øª Ø­Ù„Ù‚Ø§Øª ÙƒØ«ÙŠØ±Ø©ØŒ Ø§Ø¹Ø±Ø¶Ù‡Ø§ Ù…Ø¨Ø§Ø´Ø±Ø©
    if ep_links:
        keyboard = []
        seen = set()
        for title, href in ep_links[:60]:
            if href in seen: continue
            seen.add(href)
            keyboard.append([InlineKeyboardButton(title, callback_data=f"episode|{href}")])
        await query.message.reply_text("ğŸ“º Ø§Ø®ØªØ± Ø§Ù„Ø­Ù„Ù‚Ø©:", reply_markup=InlineKeyboardMarkup(keyboard))
        return

    # Ø®Ù„Ø§Ù Ø°Ù„Ùƒ: Ø§ÙØªØ±Ø¶ Ø£Ù†Ù‡ ÙÙŠÙ„Ù… Ø£Ùˆ ØµÙØ­Ø© Ù…Ø´Ø§Ù‡Ø¯Ø©ØŒ Ø­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ iframe
    iframe_src = extract_iframe_from_html(r.text, base=BASE_URL)
    if iframe_src:
        await query.message.reply_text(f"ğŸ¬ Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø© (iframe):\n{iframe_src}")
        return

    # Ù„Ùˆ Ù„Ù… Ù†Ø¬Ø¯ Ø´ÙŠØ¡: Ø­Ø§ÙˆÙ„ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· Ø¯Ø§Ø®Ù„ Ù…Ù‚Ø·Ø¹ Ø®Ø§Øµ Ù…Ø«Ù„ 'server' Ø£Ùˆ 'watch'
    alt = None
    for a in soup.find_all("a", href=True):
        if any(k in a['href'] for k in ["/watch", "/player", "/embed", "/iframe"]):
            alt = urljoin(BASE_URL, a['href'])
            break
    if alt:
        await query.message.reply_text(f"Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø© Ù‡Ù†Ø§:\n{alt}")
        return

    await query.message.reply_text("âŒ Ù„Ù… Ø£Ø¬Ø¯ Ø±Ø§Ø¨Ø· iframe Ø£Ùˆ Ø­Ù„Ù‚Ø§Øª ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„ØµÙØ­Ø©.")


def main():
    app = Application.builder().token(TOKEN).build()

    conv_handler = ConversationHandler(
        entry_points=[CommandHandler("start", start)],
        states={
            SEARCH: [MessageHandler(filters.Regex("^ğŸ” Ø¨Ø­Ø«$"), search_request)],
            RESULTS: [MessageHandler(filters.TEXT & ~filters.COMMAND, search_results)],
        },
        fallbacks=[CommandHandler("start", start)]
    )

    app.add_handler(conv_handler)
    app.add_handler(CallbackQueryHandler(button_handler))

    print("âœ… Ø§Ù„Ø¨ÙˆØª ÙŠØ¹Ù…Ù„ Ø§Ù„Ø¢Ù†...")
    app.run_polling()


if __name__ == "__main__":
    main()
